import os
import random
import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from torch.optim import AdamW
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score, f1_score

# ──────────────────────────────────────────────────────────────────────────
# 0) Reproducibility setup ------------------------------------------------
def set_seed(seed: int = 42):
    # Python built-ins
    random.seed(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)
    # NumPy
    np.random.seed(seed)
    # PyTorch (CPU & GPU)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    # Backend flags
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark     = False
    # Force PyTorch to use only deterministic algorithms
    torch.use_deterministic_algorithms(True)

# Call at the very start
set_seed(42)

# ──────────────────────────────────────────────────────────────────────────
# 1) Load data & build keywords column ------------------------------------
DATA_PATH = "/Users/kristianmiok/Desktop/SMASH/Data/ks_discharge_LOS.csv"
df = pd.read_csv(DATA_PATH)

from rakun2 import RakunKeyphraseDetector
hyperparameters = {
    "num_keywords":   1024,
    "merge_threshold": 1.1,
    "alpha":          0.3,
    "token_prune_len": 3
}
keyword_detector = RakunKeyphraseDetector(hyperparameters)

def extract_keywords(text, num_keywords=1024):
    kws = keyword_detector.find_keywords(text, input_type="string")
    return " ".join([kw for kw, _ in kws[:num_keywords]])

df['keywords_text'] = df['text'].apply(extract_keywords)
labels = df['LOS_long'].astype(int)

print("Total samples:", len(df))
print("Class distribution:\n", df['LOS_long'].value_counts(), "\n")

# ──────────────────────────────────────────────────────────────────────────
# 2) Device & tokenizer/model prep ----------------------------------------
# For strict reproducibility on MPS you may wish to force CPU:
# device = torch.device("cpu")
device = torch.device("mps") if torch.backends.mps.is_available() else torch.device("cpu")
print("Using device:", device)

MODEL_NAME = "answerdotai/ModernBERT-base"
tokenizer  = AutoTokenizer.from_pretrained(MODEL_NAME)

# ──────────────────────────────────────────────────────────────────────────
# 3) Dataset class with 1024-length ----------------------------------------
class MedicalDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=1024):
        self.texts     = texts.tolist()
        self.labels    = labels.tolist()
        self.tokenizer = tokenizer
        self.max_len   = max_len

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        enc = self.tokenizer(
            self.texts[idx],
            max_length=self.max_len,
            truncation=True,
            padding='max_length',
            return_tensors='pt'
        )
        return {
            'input_ids':      enc['input_ids'].squeeze(0),
            'attention_mask': enc['attention_mask'].squeeze(0),
            'labels':         torch.tensor(self.labels[idx], dtype=torch.long)
        }

# ──────────────────────────────────────────────────────────────────────────
# 4) Training & evaluation helpers ----------------------------------------
def train_one_epoch(model, loader, optimizer, criterion):
    model.train()
    total_loss = 0.0
    for batch in loader:
        optimizer.zero_grad()
        input_ids      = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels         = batch['labels'].to(device)

        logits = model(input_ids=input_ids,
                       attention_mask=attention_mask).logits
        loss = criterion(logits, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(loader)

def evaluate(model, loader):
    model.eval()
    preds, labs = [], []
    with torch.no_grad():
        for batch in loader:
            input_ids      = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels         = batch['labels'].to(device)

            logits = model(input_ids=input_ids,
                           attention_mask=attention_mask).logits
            batch_preds = torch.argmax(logits, dim=1)
            preds.extend(batch_preds.cpu().numpy())
            labs.extend(labels.cpu().numpy())
    return accuracy_score(labs, preds), f1_score(labs, preds)

# ──────────────────────────────────────────────────────────────────────────
# 5) 5-Fold CV comparing original vs keywords (1024 tokens) ---------------
modalities = {
    "original": df['text'],
    "keywords": df['keywords_text']
}
results = {mode: {"acc": [], "f1": []} for mode in modalities}

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

for fold, (train_idx, val_idx) in enumerate(skf.split(df, labels), 1):
    print(f"=== Fold {fold} ===")
    # make a fresh generator per fold so DataLoader shuffles are reproducible
    loader_gen = torch.Generator().manual_seed(42 + fold)

    for mode, texts in modalities.items():
        X_train, X_val = texts.iloc[train_idx], texts.iloc[val_idx]
        y_train, y_val = labels.iloc[train_idx], labels.iloc[val_idx]

        # class weights
        counts = np.bincount(y_train)
        total  = len(y_train)
        weights = torch.tensor([
            total / (2 * counts[0]),
            total / (2 * counts[1])
        ], dtype=torch.float, device=device)
        criterion = torch.nn.CrossEntropyLoss(weight=weights)

        # DataLoaders
        train_ds = MedicalDataset(X_train, y_train, tokenizer, max_len=1024)
        val_ds   = MedicalDataset(X_val,   y_val,   tokenizer, max_len=1024)
        train_loader = DataLoader(
            train_ds,
            batch_size=4,
            shuffle=True,
            generator=loader_gen,
            num_workers=0
        )
        val_loader = DataLoader(val_ds, batch_size=8, shuffle=False)

        # fresh model & optimizer
        model = AutoModelForSequenceClassification.from_pretrained(
            MODEL_NAME, num_labels=2
        ).to(device)
        optimizer = AdamW(model.parameters(), lr=2e-6)

        # fine-tune
        for epoch in range(1, 4):
            _ = train_one_epoch(model, train_loader, optimizer, criterion)

        # evaluate
        acc, f1 = evaluate(model, val_loader)
        results[mode]["acc"].append(acc)
        results[mode]["f1"].append(f1)
        print(f"{mode.title():>9} → Acc: {acc:.4f}, F1: {f1:.4f}")
    print()

# ──────────────────────────────────────────────────────────────────────────
# 6) Summarize -------------------------------------------------------------
print("=== 5-Fold CV Summary ===")
for mode in modalities:
    accs, f1s = results[mode]["acc"], results[mode]["f1"]
    print(f"{mode.title():<9} Accuracy: {np.mean(accs):.4f} ± {np.std(accs):.4f}")
    print(f"{mode.title():<9} F1:       {np.mean(f1s):.4f} ± {np.std(f1s):.4f}\n")
